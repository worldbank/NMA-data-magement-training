{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Tools in Python\n",
    "-----\n",
    "In this tutorial, you will learn about available software packages in Python which are used by data scientists. The ideal persona for this tutorial is a person who has never used Python and is curious to explore whats available. The idea here is to introduce you to available resources in Python rather than teach you details on how to use as a specific package.  For selected packages, a simple example will be provided to demonstrate usage. Otherwise, most packages will be listed for the leaner to explore in their own time. However, we will use some of the packages in later sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Learning Outcomes](#Learning Outcomes)\n",
    "- [General Purpose Data Wrangling](#General Purpose Data Wrangling)\n",
    "    - [Pandas](#Pandas)\n",
    "    - [Numpy](#Numpy)\n",
    "    - [Apache Spark for Large scale data processing](#Apache Spark for Large scale data processing)\n",
    "- [Web Scraping](#Web Scraping)\n",
    "- [Natural Language Processing](#Natural Language Processing)\n",
    "    - [NTLK](#Navigation)\n",
    "- [Geospatial Data](#Geospatial Data)\n",
    "    - [GDAL](#GDAL)\n",
    "    - [Geopandas](#Geopandas)\n",
    "    - [Shapely](#Shapely)\n",
    "    - [Rasterio](#Rasterio)\n",
    "- [Statistical Analysis and Optimisation](#Statistical Analysis and Optimisations)\n",
    "    - [scipy](#scipy)\n",
    "    - [Numpy](#Numpy)\n",
    "    - [statsmodels](#statsmodels)\n",
    "- [Machine Learning](#Machine Learning)\n",
    "    - [sciki-learn](#sciki-learn)\n",
    "    - [Tensorflow](#Tensorflow)\n",
    "    - [Theano](#Theano)\n",
    "- [Data Visualization](#Data Visualization)\n",
    "    - [Matplotlib](#Matplotlib)\n",
    "    - [Seaborn](#SNS)\n",
    "    - [Bokeh](#Bokeh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outomes\n",
    "After going through this notebook, the leaner should:\n",
    "- Be familiar with  Python packages for the following purposes:\n",
    "    - Data wrangling\n",
    "    - Statistical data analysis\n",
    "    - Data visualization\n",
    "    - Natural language processing\n",
    "    - Geospatial data processing and visualization\n",
    "    - Machine Learning\n",
    "- Appreciate the Python package ecoystem for data science\n",
    "- Use import statement to import packages and perfom simple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Data Wrangling\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this category, we look at Python libraries which can be used to perfom the most common data wrangling tasks such as data ingestion, data cleaning, subsetting data, recoding variables, checking for missing values, imputing missing values, exploring distributions and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Pandas is a library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. Pandas is free software released under the three-clause BSD license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data Structures in Pandas\n",
    "Pandas has two mainndata structures as follows: Series, DataFrames and Panels.\n",
    "1. **Series:** A 1-dimensional labelled array that can hold data of any type (integer, string, float, python objects, etc.). \n",
    "    Itâ€™s axis labels are collectively called an index.\n",
    "2. **DataFrame:** A 2-dimensional labelled data structure with columns and both a row and a column index. A dataframe can \n",
    "    be used to represent 3-D data using multiindexing. \n",
    "    \n",
    "A dataframe is often abbreviated as ```df```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Dunstan','Mercy','Khama','Lara','Khali','Gloria']\n",
    "names = pd.Series(data)\n",
    "print (names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"name\":['Dunstan','Mercy','Khama','Lara','Khali','Gloria']}\n",
    "names = pd.Series(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"name\":['Dunstan','Mercy','Khama','Lara','Khali','Gloria'],\n",
    "       \"age\": [100, 20, 7, 11, 30, 88]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion with Pandas: A Case of CSV file\n",
    "Pandas can work with many data stores and file formats. To take a quick look at what file formats pandas can read, type ```pd.read``` and then hit tab, you should see a list of all the file formats supported by pandas. In this example, we show reading from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.rea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas package \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the name of the CSV file\n",
    "data_file = '../../day2-python-for-data-science/data/power-outages.csv'\n",
    "\n",
    "# Read data into dataframe\n",
    "# dataframe is the pandas object for handling tabular data\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [{'name':'Dunstan', 'age':50}, {'name':'Khama', 'age':1}]\n",
    "df_from_list = pd.DataFrame(my_data)\n",
    "df_from_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "Once you have read the data, pandas has many functions to allow you explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the n-top rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes and other info about the columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics if it makes sense\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "print(type(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps with Pandas\n",
    "Please refer to pandas [documentation](http://pandas.pydata.org/pandas-docs/stable/10min.html) for tutorials on how to perfom various tasks such as indexing rows, subsetting the data, chaning column names and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Numpy** is one of the underlying libraries which powers many high level packages such as pandas. In most case, you will not need to interact with **Numpy** directly but its an essential package for data manipulation and scientific computing in Python. Its useful for linear algebra, Fourier transform, and random number capabilities because it has advanced array/matrix functionalities. Also, most of the machine learning packages do require input as **Numpy** arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays in Numpy\n",
    "In the code below, we show how **Numpy** arrays work seamlessly with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# create two arrays\n",
    "x = np.array([i + 100 for i in range(10)])\n",
    "y = np.array([i -10 for i in range(10)])\n",
    "\n",
    "# create a dictionary with key as column name and value as the numpy arrays\n",
    "data = {\"x\": x, \"y\":y}\n",
    "\n",
    "# Use the dictinary to create a pandas dataframe\n",
    "df_from_np = pd.DataFrame(data)\n",
    "\n",
    "# Check out the dataframe\n",
    "df_from_np.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array([1,2,3])\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([i + 100 for i in range(10)])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out Numpy Documentation for Additional Details\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "[Numpy documentation](https://docs.scipy.org/doc/numpy/user/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallell Data Processing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When you have large datasets regular packages such as pandas do not scale very well. For instance, \n",
    "even a 5gb dataset can't be handled gracefully by pandas. When you have these large datasets, \n",
    "you look for packages which can parellilze processing and make things faster. In Python, they \n",
    "are many tools for parallell data processing as follows:\n",
    "- [Apache Spark](https://spark.apache.org): this probably the most popular framework for distributed data processing\n",
    "- [Dask](http://docs.dask.org/en/latest/why.html): This library offers code parallelisation \n",
    "    utilising existing libraries such as pandas\n",
    "- [Python Multiprocessing Library](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process): Python offers a multiprocessing library but it requires people \n",
    "    who are more experienced with programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Power of Apache Spark\n",
    "In the example below, lets see how fast things can get when we use Apache Spark compared to Pandas. Please pay attention to the time taken to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# A utility function for measuring execution \n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timefn(fn):\n",
    "    @wraps(fn)\n",
    "    def measure_time(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = fn(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(\"@timefn:\" + fn.__name__ + \" took \" + str(t2 - t1) + \" seconds\")\n",
    "        return result\n",
    "    return measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# we add @timefn to measure execution time of the function\n",
    "@timefn\n",
    "def load_big_csv_with_apache_spark(big_csv=None):\n",
    "    \"\"\"\n",
    "    A simple function which loads a CSV file using Apache Spark and\n",
    "    then counts how many rows are in the file\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.master(\"local[12]\").appName(\"data_processor\").getOrCreate()\n",
    "    df = spark.read.csv(big_csv)\n",
    "    cnt = df.count()\n",
    "    print('Number of rows in big CSV file: {:,}'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@timefn\n",
    "def load_big_csv_with_pandas(big_csv=None):\n",
    "    \"\"\"\n",
    "    Use pandas library to load the large CSV\n",
    "    \"\"\"\n",
    "    # Read CSV as a dataframe (df) here\n",
    "    df = pd.read_csv(big_csv)\n",
    "    \n",
    "    # Get the total number of rows\n",
    "    cnt = df.shape[0]\n",
    "    print('Done with counting')\n",
    "    \n",
    "    # Get total number of unique activities using 'SID' column using code below\n",
    "    uniq_SID = list(df['SID'].unique())\n",
    "    print('Done with unique values')\n",
    "    \n",
    "    # Use len() function to get number of elements in the list above\n",
    "    cnt_uniq = len(uniq_SID)\n",
    "    \n",
    "    # print out the results\n",
    "    print('Number of rows in big CSV file: {:,}, Number of unique activities: {}'.format(cnt,cnt_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import APache Spark relevant library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# the task is to read in an 8GB CSV file and then count number of rows\n",
    "big_csv_file = os.path.abspath(\"/Users/dmatekenya/Google-Drive/gigs/un-siap-2018/ch3-big-data-processing/data/activity_log_raw.csv\")\n",
    "load_big_csv_with_apache_spark(big_csv=big_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using pandas\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "print('Pandas starting now {}'.format(start.ctime()))\n",
    "load_big_csv_with_pandas(big_csv=big_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We often have to deal with data which is geospatial in nature or has geographic coordinates. Python has a suite of tools for handling spatial data as we will show in the next section.\n",
    "- [GDAL](https://www.gdal.org): this is one of the core libraries for provising geospatial data functionality. Most of the other libraries are based on GDAL.\n",
    "- [Geopandas](http://geopandas.org): A pandas version for geospatial data\n",
    "- [Shapely](https://shapely.readthedocs.io/en/stable/manual.html): yet another spatial data library with alot of functionalities.\n",
    "- [Rasterio](https://rasterio.readthedocs.io/en/latest/quickstart.html): this library fosues on raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Shapefiles with GDAL OGR Library\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "GDAL s a translator library for raster and vector geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. Its also used in oen source GIS software such as QGIS. In Python, you can use the library to read raster and vector data as well as manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from osgeo import ogr\n",
    "\n",
    "# shapefile path\n",
    "shp_file = '../../day2-python-for-data-science/data/Balaka_EA.shp'\n",
    "\n",
    "# open shapefile\n",
    "file = ogr.Open(shp_file)\n",
    "shape = file.GetLayer(0)\n",
    "\n",
    "#first feature of the shapefile\n",
    "feature = shape.GetFeature(0)\n",
    "\n",
    "# Convert first feature to GeoJSON\n",
    "first = feature.ExportToJson()\n",
    "\n",
    "# print the JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shp_file)\n",
    "gdf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Spatial Data with Geopandas\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# read the shapefile which we arleady defined in cells above\n",
    "df = gpd.read_file(shp_file)\n",
    "\n",
    "# diplay the shapefile \n",
    "display(df.plot(figsize=(8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Geospatial Data Libraries\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "They are other essential packages to handle spatial data in Python including:\n",
    "- [Fiona](https://fiona.readthedocs.io/en/stable/): Geopandas uses this library behind the scenes\n",
    "- [Descartes](https://bitbucket.org/sgillies/descartes/): For spatial data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Optimisations\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are interested in more sophisticated statistical analysis, Python also offers further tools in addtion to packages like Pandas which also has basic statistical functionalities. Here, we look at the following packages:\n",
    "- [Statsmodels](https://www.statsmodels.org/stable/index.html): For more rigorous statistical analysis than found in Pandas\n",
    "- [Scipy](): This is part of the Python scientific computing stack which also includes Numpy\n",
    "- [Numpy](): Arleady mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Estimation with StatsModels\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.Refer to [documentation](https://www.statsmodels.org/stable/index.html) for further details. The exampleã€€below has been adopted from StatsModels documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "nsample = 1000\n",
    "x = np.linspace(0, 10, 1000)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 0.1, 10])\n",
    "e = np.random.normal(size=nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python has become the lingua franca for many data science applications including Machine Learning. It therefore comes as no surprise that its one of the leading scripting langueages when it comes to machine learning. The top machine learning packages in Python based on usage include:\n",
    "- [scikit-learn](https://scikit-learn.org/stable/): This is a general purpose machine learning library and it can be used for diverse machine tasks unlike other libraries which may be more specialised. \n",
    "- [Tensorflow](https://www.tensorflow.org): this library is focused on deep learning and other neural networks approaches\n",
    "- [Keras](http://keras.io): Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow and Theano\n",
    "- [Theano](http://deeplearning.net/software/theano/): Also focused on deep learning and neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "A lot of unstructured data is available on the internet. Once gathered and put into a structured & meaningful format, this data can be used to perform analytics and derive meaningful insights. In this tutorial, I am going to demonstrate how we can collect such data from the internet and process it to get it to be in a structured format. This process is known as Web Scraping. It is also called Screen Scraping, Web Harvesting or Web Data Extraction. In Python, they are many packages for web scraping but the most common ones include:\n",
    "- [requests](http://docs.python-requests.org/en/master/): A library that fetches the content of the URL.\n",
    "- [Beatifulsoup](https://www.crummy.com/software/BeautifulSoup/): A library that allows you to parse the HTML source code in a beautiful way.\n",
    "- [Scrapy](https://scrapy.org): An out of the box solution for web scraping with other advanced functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sneak Peak at Requests\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = \"https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html\"\n",
    "r = requests.get(web_url)\n",
    "c = r.content\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use request to get contents od webpage\n",
    "r = requests.get(\"https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html\")\n",
    "c = r.content\n",
    "\n",
    "# And BS to parse the html\n",
    "soup = BeautifulSoup(c,\"html.parser\")\n",
    "\n",
    "# we print a heading three tag\n",
    "most_pop_last_week = soup.find('h3').text\n",
    "print(most_pop_last_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Simply and in short, natural language processing (NLP) is about developing applications and services that are able to understand human languages. We are talking here about practical examples of natural language processing (NLP) like speech recognition, speech translation, understanding complete sentences, understanding synonyms of matching words, and writing complete grammatically correct sentences and paragraphs.\n",
    "\n",
    "In Python, there are many libraries for dealing with language and textual data, we preview some of them:\n",
    " - [Natural language toolkit (NLTK](https://www.nltk.org): A library for general purpose processing and analysis of human language data.\n",
    " - [TextBlob](https://textblob.readthedocs.io/en/dev/):Built on top of NLTK above, this package provides similar natural language functionality\n",
    " - [spaCy](https://spacy.io): Optmised for large scale processing of natural language data.\n",
    " - [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/): An old NLP package written in Java but it has some Python APIs such as [py-corenlp](https://github.com/smilli/py-corenlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A taste of NLTK on Twitter Data\n",
    "Lets quickly look at this twitter dataset from [datahack](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/) just to demonstrate how language processing is possible in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we have to import the package\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# When running for the first time, please uncomment line beloe to donwload the extra packages\n",
    "nltk.download('punkt') \n",
    "\n",
    "\n",
    "# path to data file\n",
    "twitter_data = '../../day2-python-for-data-science/data/twitter-dataset-from-datahack.csv'\n",
    "\n",
    "# Get CSV\n",
    "df_twitter = pd.read_csv(twitter_data)\n",
    "\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.iloc[0].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One preprocessing steps in NLP is tokenizing (converting sentences into separate words)\n",
    "df_twitter['tokens'] = df_twitter.apply(lambda x: nltk.word_tokenize(x['tweet']), axis=1)\n",
    "\n",
    "# Lets check the split tweets\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.iloc[0].tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Translation with TextBlob\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Translation is one of the common tasks in NLP. Many APIs provide translation, for example, you can use Google API to tranlate texts on demand for free or for a charge depending on the bulk of your translations. However, in Python the NLP libraries also enable you to translate as we will see in exmaple below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install nltk\n",
    "pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "chinese_blob = TextBlob(u\"ç¾Žä¸½ä¼˜äºŽä¸‘é™‹\")\n",
    "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_doc_title(file=None):\n",
    "    try:\n",
    "        open_file = open(file, mode='rb')\n",
    "        if file[-3:] == \"pdf\":\n",
    "            pdf_document = PyPDF2.PdfFileReader(open_file)\n",
    "            doc_info = pdf_document.documentInfo\n",
    "            title = doc_info[\"/Title\"]\n",
    "\n",
    "            title_relevance = \"No\"\n",
    "            for w in KEY_WORDS:\n",
    "                if w in title.lower():\n",
    "                    title_relevance = \"Yes\"\n",
    "                    break\n",
    "            return {\"docTitle\": title, \"titleRelevant\": title_relevance}\n",
    "        else:\n",
    "            return {\"docTitle\": \"TBD\", \"titleRelevant\": \"TBD\"}\n",
    "    except Exception:\n",
    "        return {\"docTitle\": \"TBD\", \"titleRelevant\": \"TBD\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_dir = Path.cwd().parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python has many data visualisation tools, here are some of them:\n",
    "- [Matplotlib]() : this is the base plotting engine in Python. Most of the other packages are somehow based on it.\n",
    "- [Seaborn](): A relatively new packahe with more colourful plots\n",
    "- [Bokeh](): this is an interactive visualization library that targets modern web browsers for presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualisations with Bokeh\n",
    "\n",
    "The simple interactive example below has been copied from [Bokeh Documentation] (https://demo.bokehplots.com/apps/moviesfrom) for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file\n",
    "import numpy as np\n",
    "N = 4000\n",
    "x = np.random.random(size=N) * 100\n",
    "y = np.random.random(size=N) * 100\n",
    "radii = np.random.random(size=N) * 1.5\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n",
    "]\n",
    "\n",
    "TOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\"\n",
    "\n",
    "p = figure(tools=TOOLS)\n",
    "\n",
    "p.scatter(x, y, radius=radii,\n",
    "          fill_color=colors, fill_alpha=0.6,\n",
    "          line_color=None)\n",
    "\n",
    "output_file(\"color_scatter.html\", title=\"color_scatter.py example\")\n",
    "\n",
    "# please inspect the plot in the browser\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this tutorial, we previewed some of the essential packages in Python for doing data science with the intent to get excited to learn more. You can always learn more about any of the packages mentioned here by going to the documentation page linked or doing a Google search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "326px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
